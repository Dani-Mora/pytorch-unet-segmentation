{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "The purpose of this notebook is to scrap products form real retail websites in order to evaluate approaches on product retrieval by match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from time import sleep\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "import requests\n",
    "import urllib.request\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each product:\n",
    "\n",
    "- We use first product image found\n",
    "- We select all possible colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Product(object):\n",
    "    \n",
    "    url: str\n",
    "    image_url: str\n",
    "    name: str\n",
    "    section: str\n",
    "        \n",
    "    def from_url(product_url: str, section: str) -> Product:\n",
    "        \"\"\" Returns product from url. Returns None on error\"\"\"\n",
    "        response = requests.get(product_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return Product(url=product_url,\n",
    "                       section=section,\n",
    "                       name=name_in_product_page(soup).text,\n",
    "                       image_url=images_in_product_page(soup)[0].get('src'))\n",
    "\n",
    "\n",
    "def name_in_product_page(product_soup: BeautifulSoup) -> str:\n",
    "    return product_soup.find('span', {'class': 'nj-namecomponent-name'})\n",
    "        \n",
    "        \n",
    "def images_in_product_page(product_soup: BeautifulSoup) -> List[Tag]:\n",
    "    return product_soup.findAll(\n",
    "        'img',\n",
    "        {\n",
    "            'class': 'aino-image',\n",
    "            'src': lambda x: x is not None and 'nudiejeans' in x\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def color_urls(product_url: str, root_url: str) -> List[str]:\n",
    "    \"\"\" Returns pages for all available colors from product \"\"\"\n",
    "    response = requests.get(product_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    color_links = soup.findAll('a', {'class': '_19uix'})\n",
    "    return [\n",
    "        f'{root_url}/{color_link.get(\"href\")}'\n",
    "        for color_link in color_links\n",
    "    ]\n",
    "\n",
    "\n",
    "def listing_to_products(listing_tag: Tag,\n",
    "                        section: str,\n",
    "                        root_url: str = 'https://www.nudiejeans.com',\n",
    "                        sleep_between_products_ms: int = 100) -> List[Product]:\n",
    "    path = listing_tag.find('a').get('href')\n",
    "    url = f'{root_url}/{path}'\n",
    "    all_colors = color_urls(url, root_url)\n",
    "    \n",
    "    if len(all_colors) == 0:\n",
    "        all_colors = [url]\n",
    "\n",
    "    products = []\n",
    "    sleep_seconds = sleep_between_products_ms / 1000.0\n",
    "    for color_url in all_colors:\n",
    "        products.append(Product.from_url(color_url, section))\n",
    "        sleep(sleep_seconds)\n",
    "\n",
    "    return products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = [\n",
    "    'https://www.nudiejeans.com/jeans',\n",
    "    'https://www.nudiejeans.com/selection/pants',\n",
    "    'https://www.nudiejeans.com/selection/shorts',\n",
    "    'https://www.nudiejeans.com/selection/denim-jackets',\n",
    "    'https://www.nudiejeans.com/selection/jackets',\n",
    "    'https://www.nudiejeans.com/selection/shirts'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse products from listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Products from jeans: 100%|██████████| 102/102 [06:10<00:00,  3.63s/it]\n",
      "Products from pants: 100%|██████████| 18/18 [00:57<00:00,  3.22s/it]\n",
      "Products from shorts: 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]\n",
      "Products from denim-jackets: 100%|██████████| 27/27 [01:32<00:00,  3.44s/it]\n",
      "Products from jackets: 100%|██████████| 24/24 [00:42<00:00,  1.75s/it]\n",
      "Products from shirts: 100%|██████████| 44/44 [01:58<00:00,  2.70s/it]\n"
     ]
    }
   ],
   "source": [
    "def section_to_products(section_url) -> List[Product]:\n",
    "    response =  requests.get(section_url)\n",
    "    section_soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    section_name = section_url.split('/')[-1]\n",
    "    \n",
    "    listings = section_soup.findAll('div', {'class': '_1WLsJ'})\n",
    "    products = [\n",
    "        listing_to_products(listing, section_name)\n",
    "        for listing in tqdm(listings, desc=f'Products from {section_name}')\n",
    "    ]\n",
    "    \n",
    "    return list(itertools.chain(*products))\n",
    "\n",
    "all_products = [\n",
    "    section_to_products(section_url)\n",
    "    for section_url in sections\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten list of products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products = list(itertools.chain(*all_products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 790 products\n"
     ]
    }
   ],
   "source": [
    "print(f'We have {len(all_products)} products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('products.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_products, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
